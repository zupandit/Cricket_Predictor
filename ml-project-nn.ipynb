{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11302346,"sourceType":"datasetVersion","datasetId":7068334},{"sourceId":231971275,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data Engineering and Feature Enhancement\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom imblearn.over_sampling import SMOTE","metadata":{"_uuid":"37699e6e-635e-4263-a2fe-ace85cfd070e","_cell_guid":"2b08cd54-5062-49a8-811f-2a60ee27ec38","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-06T23:05:25.078223Z","iopub.execute_input":"2025-04-06T23:05:25.078638Z","iopub.status.idle":"2025-04-06T23:05:25.084016Z","shell.execute_reply.started":"2025-04-06T23:05:25.078604Z","shell.execute_reply":"2025-04-06T23:05:25.082577Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Load dataset (limit to first 2000 rows for speed, adjust as needed)\ndata = pd.read_csv(\"/kaggle/input/cricket/over_features.csv\")\ndata = data.iloc[:2000]\n\n# Check for missing values and drop any rows with missing values\nprint(\"Missing values in data:\")\nprint(data.isnull().sum())\ndata.dropna(inplace=True)\n\n# Remove identifier\ndata = data.drop(columns=[\"match_id\"])\n\n# Encode categorical columns: 'team' and 'match_phase'\nteam_mapping = {team: idx for idx, team in enumerate(data[\"team\"].unique())}\ndata[\"team_encoded\"] = data[\"team\"].map(team_mapping)\nphase_mapping = {phase: idx for idx, phase in enumerate(data[\"match_phase\"].unique())}\ndata[\"match_phase_encoded\"] = data[\"match_phase\"].map(phase_mapping)\n\n# Feature Engineering: Add enhanced features\ndata[\"pressure_index\"] = data[\"dot_ball_pressure\"] * data[\"required_desired_run_rate\"]\ndata[\"wicket_pressure\"] = data[\"number_of_wickets_lost\"] * data[\"required_desired_run_rate\"]\ndata[\"late_over_flag\"] = (data[\"over\"] > 15).astype(int)\ndata[\"bowler_pressure\"] = data[\"current_bowler_economy\"] * (data[\"bowler_wickets_in_match\"] + 1)\ndata[\"aggressiveness_index\"] = data[\"striker_strike_rate\"] * (data[\"striker_boundaries_hit\"] + 1)\n\n# Drop the original categorical columns if not needed further\ndata = data.drop(columns=[\"team\", \"match_phase\"])\n\n# Define features and target\nX = data.drop(columns=[\"wicket_next_over\"])\ny = data[\"wicket_next_over\"]\n\n# Train-Test Split (using stratification)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Scale features using StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nsmote = SMOTE(random_state=42)\nX_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T23:05:25.085898Z","iopub.execute_input":"2025-04-06T23:05:25.086238Z","iopub.status.idle":"2025-04-06T23:05:25.304142Z","shell.execute_reply.started":"2025-04-06T23:05:25.086208Z","shell.execute_reply":"2025-04-06T23:05:25.303098Z"}},"outputs":[{"name":"stdout","text":"Missing values in data:\nmatch_id                             0\ninnings                              0\nteam                                 0\nover                                 0\nballs_faced_by_striker               0\nstriker_strike_rate                  0\nstriker_boundaries_hit               0\ndot_ball_pressure                    0\ncurrent_bowler_economy               0\nbowler_wickets_in_match              0\ntotal_overs_completed                0\novers_since_last_wicket              0\nnumber_of_wickets_lost               0\nrequired_desired_run_rate            0\ncurrent_run_rate                     0\nwickets_lost_last_3_overs            0\nnumber_of_boundaries_last_3_overs    0\nnumber_of_dot_balls_last_over        0\npowerplay_overs_remaining            0\nmatch_phase                          0\nwicket_next_over                     0\ndtype: int64\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Define evaluation metric function\ndef calculate_metrics(y_true, y_pred):\n    acc = accuracy_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    return acc, prec, rec, f1\n\n# Neural Network Implementation with PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Determine input dimension from our feature-enhanced data\ninput_dim = X_train.shape[1]\n\nclass ImprovedWicketPredictor(nn.Module):\n    def __init__(self, input_dim):\n        super(ImprovedWicketPredictor, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 16)\n        self.fc4 = nn.Linear(16, 8)\n        self.fc5 = nn.Linear(8, 1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.dropout = nn.Dropout(p=0.5)\n        self.batch_norm1 = nn.BatchNorm1d(64)\n        self.batch_norm2 = nn.BatchNorm1d(32)\n        self.batch_norm3 = nn.BatchNorm1d(16)\n        self.batch_norm4 = nn.BatchNorm1d(8)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.batch_norm1(x)\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        x = self.batch_norm2(x)\n        x = self.dropout(x)\n        x = self.relu(self.fc3(x))\n        x = self.batch_norm3(x)\n        x = self.dropout(x)\n        x = self.relu(self.fc4(x))\n        x = self.batch_norm4(x)\n        x = self.fc5(x)\n        x = self.sigmoid(x)\n        return x\n\ndef NN_predict(X_train, X_test, Y_train, Y_test, epochs=100, patience=10):\n    # Convert training and test data to PyTorch tensors\n    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n    y_train_tensor = torch.tensor(Y_train.to_numpy(), dtype=torch.float32).view(-1, 1)\n    y_test_tensor = torch.tensor(Y_test.to_numpy(), dtype=torch.float32).view(-1, 1)\n    \n    # Create DataLoader for the training data\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    \n    # Instantiate the model, loss function, and optimizer\n    model = ImprovedWicketPredictor(input_dim)\n    criterion = nn.BCELoss()\n    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n    \n    best_accuracy = 0\n    no_improvement = 0\n    \n    # Training loop with early stopping\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        # Evaluate on the training set for early stopping\n        model.eval()\n        with torch.no_grad():\n            y_train_pred = model(X_train_tensor).round()\n            train_acc = accuracy_score(y_train_tensor.numpy(), y_train_pred.numpy())\n        \n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_acc:.4f}\")\n        \n        if train_acc > best_accuracy:\n            best_accuracy = train_acc\n            no_improvement = 0\n        else:\n            no_improvement += 1\n        \n        if no_improvement >= patience:\n            print(\"Early stopping triggered.\")\n            break\n    \n    # Final evaluation on the test set\n    model.eval()\n    with torch.no_grad():\n        y_test_pred = model(X_test_tensor).round()\n    \n    return y_test_pred, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T23:05:25.305830Z","iopub.execute_input":"2025-04-06T23:05:25.306141Z","iopub.status.idle":"2025-04-06T23:05:25.325831Z","shell.execute_reply.started":"2025-04-06T23:05:25.306116Z","shell.execute_reply":"2025-04-06T23:05:25.324520Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Run the Neural Network on the feature-enhanced, scaled data\ny_test_pred, nn_model = NN_predict(X_train_scaled, X_test_scaled, y_train, y_test)\nacc, prec, rec, f1 = calculate_metrics(y_test, y_test_pred.numpy())\nprint(\"\\nNeural Network Test Metrics:\")\nprint(f\" Accuracy: {acc:.4f}\")\nprint(f\" Precision: {prec:.4f}\")\nprint(f\" Recall: {rec:.4f}\")\nprint(f\" F1 Score: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T23:05:25.326973Z","iopub.execute_input":"2025-04-06T23:05:25.327308Z","iopub.status.idle":"2025-04-06T23:05:28.537439Z","shell.execute_reply.started":"2025-04-06T23:05:25.327264Z","shell.execute_reply":"2025-04-06T23:05:28.536153Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100, Loss: 0.6818, Train Accuracy: 0.7000\nEpoch 2/100, Loss: 0.6550, Train Accuracy: 0.7181\nEpoch 3/100, Loss: 0.6267, Train Accuracy: 0.7194\nEpoch 4/100, Loss: 0.6241, Train Accuracy: 0.7225\nEpoch 5/100, Loss: 0.6105, Train Accuracy: 0.7275\nEpoch 6/100, Loss: 0.6060, Train Accuracy: 0.7262\nEpoch 7/100, Loss: 0.6018, Train Accuracy: 0.7256\nEpoch 8/100, Loss: 0.5908, Train Accuracy: 0.7250\nEpoch 9/100, Loss: 0.5968, Train Accuracy: 0.7244\nEpoch 10/100, Loss: 0.5907, Train Accuracy: 0.7244\nEpoch 11/100, Loss: 0.5922, Train Accuracy: 0.7238\nEpoch 12/100, Loss: 0.5911, Train Accuracy: 0.7238\nEpoch 13/100, Loss: 0.5960, Train Accuracy: 0.7238\nEpoch 14/100, Loss: 0.5901, Train Accuracy: 0.7225\nEpoch 15/100, Loss: 0.5863, Train Accuracy: 0.7231\nEarly stopping triggered.\n\nNeural Network Test Metrics:\n Accuracy: 0.7225\n Precision: 0.5000\n Recall: 0.0180\n F1 Score: 0.0348\n","output_type":"stream"}],"execution_count":16}]}