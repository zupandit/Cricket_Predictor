{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11279983,"sourceType":"datasetVersion","datasetId":7052194}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, StackingClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.naive_bayes import GaussianNB\nfrom imblearn.over_sampling import SMOTE\n\n# ------------------------------\n# 1. Data Preprocessing\n# ------------------------------\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/cricket-predictor/over_features.csv')\n\n# Create engineered features\ndf['pressure_index'] = df['dot_ball_pressure'] * df['required_desired_run_rate']\ndf['wicket_pressure'] = df['number_of_wickets_lost'] * df['required_desired_run_rate']\ndf['late_over_flag'] = (df['over'] > 15).astype(int)\ndf['bowler_pressure'] = df['current_bowler_economy'] * (df['bowler_wickets_in_match'] + 1)\ndf['aggressiveness_index'] = df['striker_strike_rate'] * (df['striker_boundaries_hit'] + 1)\n\n# Drop unneeded columns\ndf = df.drop(columns=['match_id'])\n\n# One-hot encode categorical columns if needed\ndf = pd.get_dummies(df, columns=['team', 'match_phase'], drop_first=True)\n\n# Separate features and target (target: wicket_next_over)\nX = df.drop(columns=['wicket_next_over'])\ny = df['wicket_next_over']\n\n# ------------------------------\n# 2. Train-Test Split & Scaling\n# ------------------------------\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# ------------------------------\n# 2.5 Addressing Class Imbalance with SMOTE\n# ------------------------------\nsmote = SMOTE(random_state=42)\nX_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n\nprint(\"Class distribution before SMOTE:\", np.bincount(y_train))\nprint(\"Class distribution after SMOTE:\", np.bincount(y_train_balanced))\n\n# ------------------------------\n# 3. Define Evaluation Helper Function\n# ------------------------------\n# This function computes metrics on both training and test data.\ndef evaluate_model_full(model, model_name, X_train, y_train, X_test, y_test, threshold=0.5):\n    # Training predictions\n    if hasattr(model, \"predict_proba\"):\n        y_train_prob = model.predict_proba(X_train)[:, 1]\n    else:\n        y_train_prob = model.predict(X_train)\n    y_train_pred = (y_train_prob >= threshold).astype(int)\n    train_metrics = {\n        'Accuracy': accuracy_score(y_train, y_train_pred),\n        'Precision': precision_score(y_train, y_train_pred, zero_division=0),\n        'Recall': recall_score(y_train, y_train_pred, zero_division=0),\n        'F1 Score': f1_score(y_train, y_train_pred, zero_division=0),\n        'ROC-AUC': roc_auc_score(y_train, y_train_prob) if hasattr(model, \"predict_proba\") else None\n    }\n    \n    # Test predictions\n    if hasattr(model, \"predict_proba\"):\n        y_test_prob = model.predict_proba(X_test)[:, 1]\n    else:\n        y_test_prob = model.predict(X_test)\n    y_test_pred = (y_test_prob >= threshold).astype(int)\n    test_metrics = {\n        'Accuracy': accuracy_score(y_test, y_test_pred),\n        'Precision': precision_score(y_test, y_test_pred, zero_division=0),\n        'Recall': recall_score(y_test, y_test_pred, zero_division=0),\n        'F1 Score': f1_score(y_test, y_test_pred, zero_division=0),\n        'ROC-AUC': roc_auc_score(y_test, y_test_prob) if hasattr(model, \"predict_proba\") else None\n    }\n    \n    print(f\"{model_name} Train Metrics: {train_metrics}\")\n    print(f\"{model_name} Test Metrics: {test_metrics}\\n\")\n    return train_metrics, test_metrics\n\n# Dictionaries to store metrics for all models\nresults_train = {}\nresults_test = {}\n\n# ------------------------------\n# 3. Model Training and Evaluation\n# ------------------------------\n\n# 3.1 Linear Regression (as a baseline classifier)\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_balanced, y_train_balanced)\n# For linear regression, we use a threshold of 0.5 on predictions.\ntrain_m, test_m = evaluate_model_full(lin_reg, \"Linear Regression\", X_train_scaled, y_train, X_test_scaled, y_test)\nresults_train[\"Linear Regression\"] = train_m\nresults_test[\"Linear Regression\"] = test_m\n\n# 3.2 Logistic Regression\nlog_reg = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\nlog_reg.fit(X_train_balanced, y_train_balanced)\ntrain_m, test_m = evaluate_model_full(log_reg, \"Logistic Regression\", X_train_scaled, y_train, X_test_scaled, y_test)\nresults_train[\"Logistic Regression\"] = train_m\nresults_test[\"Logistic Regression\"] = test_m\n\n# 3.3 K-Nearest Neighbors (KNN)\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_balanced, y_train_balanced)\ntrain_m, test_m = evaluate_model_full(knn, \"KNN\", X_train_scaled, y_train, X_test_scaled, y_test)\nresults_train[\"KNN\"] = train_m\nresults_test[\"KNN\"] = test_m\n\n# 3.4 Decision Tree\ntree = DecisionTreeClassifier(class_weight='balanced', random_state=42)\ntree.fit(X_train_balanced, y_train_balanced)\ntrain_m, test_m = evaluate_model_full(tree, \"Decision Tree\", X_train_scaled, y_train, X_test_scaled, y_test)\nresults_train[\"Decision Tree\"] = train_m\nresults_test[\"Decision Tree\"] = test_m\n\n# 3.6 Random Forest\nrf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\nrf.fit(X_train_balanced, y_train_balanced)\ntrain_m, test_m = evaluate_model_full(rf, \"Random Forest\", X_train_scaled, y_train, X_test_scaled, y_test)\nresults_train[\"Random Forest\"] = train_m\nresults_test[\"Random Forest\"] = test_m\n\n# 3.7 Ensemble Method: Voting Classifier\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_reg), ('tree', tree), ('rf', rf)],\n    voting='soft'\n)\nvoting_clf.fit(X_train_balanced, y_train_balanced)\ntrain_m, test_m = evaluate_model_full(voting_clf, \"Voting Classifier\", X_train_scaled, y_train, X_test_scaled, y_test)\nresults_train[\"Voting Classifier\"] = train_m\nresults_test[\"Voting Classifier\"] = test_m\n\n# 3.8 K-Means Clustering (Unsupervised)\nkmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\nkmeans.fit(X_train_balanced)\nclusters_train = kmeans.labels_\n# Map clusters to the majority class in the balanced training data\nmapping = {}\nfor cluster in np.unique(clusters_train):\n    indices = np.where(clusters_train == cluster)[0]\n    majority_class = y_train_balanced.iloc[indices].mode()[0]\n    mapping[cluster] = majority_class\nclusters_test = kmeans.predict(X_test_scaled)\ny_pred_kmeans = np.array([mapping[cluster] for cluster in clusters_test])\n# For KMeans we evaluate manually since there's no predict_proba\ntrain_pred_kmeans = kmeans.labels_\n# We'll compute train metrics using the mapping on training data:\ny_train_pred_kmeans = np.array([mapping[cluster] for cluster in train_pred_kmeans])\ntrain_metrics_kmeans = {\n    'Accuracy': accuracy_score(y_train_balanced, y_train_pred_kmeans),\n    'Precision': precision_score(y_train_balanced, y_train_pred_kmeans, zero_division=0),\n    'Recall': recall_score(y_train_balanced, y_train_pred_kmeans, zero_division=0),\n    'F1 Score': f1_score(y_train_balanced, y_train_pred_kmeans, zero_division=0),\n    'ROC-AUC': None\n}\ntest_metrics_kmeans = {\n    'Accuracy': accuracy_score(y_test, y_pred_kmeans),\n    'Precision': precision_score(y_test, y_pred_kmeans, zero_division=0),\n    'Recall': recall_score(y_test, y_pred_kmeans, zero_division=0),\n    'F1 Score': f1_score(y_test, y_pred_kmeans, zero_division=0),\n    'ROC-AUC': None\n}\nprint(\"KMeans Clustering Train Metrics:\", train_metrics_kmeans)\nprint(\"KMeans Clustering Test Metrics:\", test_metrics_kmeans, \"\\n\")\nresults_train[\"KMeans Clustering\"] = train_metrics_kmeans\nresults_test[\"KMeans Clustering\"] = test_metrics_kmeans\n\n# Gaussian Naive Bayes\nnb_model = GaussianNB()\nnb_model.fit(X_train_balanced, y_train_balanced)\ntrain_m, test_m = evaluate_model_full(nb_model, \"Gaussian Naive Bayes\", X_train_scaled, y_train, X_test_scaled, y_test)\nresults_train[\"Gaussian Naive Bayes\"] = train_m\nresults_test[\"Gaussian Naive Bayes\"] = test_m\n\n# AdaBoost\nada_model = AdaBoostClassifier(\n    estimator=DecisionTreeClassifier(max_depth=1, class_weight='balanced', random_state=42),\n    n_estimators=50,\n    random_state=42\n)\nada_model.fit(X_train_balanced, y_train_balanced)\ntrain_m, test_m = evaluate_model_full(ada_model, \"AdaBoost\", X_train_scaled, y_train, X_test_scaled, y_test)\nresults_train[\"AdaBoost\"] = train_m\nresults_test[\"AdaBoost\"] = test_m\n\n# Stacking Classifier\nstacking_estimators = [\n    ('lr', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)),\n    ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n    ('rf', RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42))\n]\nstacking_model = StackingClassifier(\n    estimators=stacking_estimators,\n    final_estimator=LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n)\nstacking_model.fit(X_train_balanced, y_train_balanced)\ntrain_m, test_m = evaluate_model_full(stacking_model, \"Stacking Classifier\", X_train_scaled, y_train, X_test_scaled, y_test)\nresults_train[\"Stacking Classifier\"] = train_m\nresults_test[\"Stacking Classifier\"] = test_m\n\n# (Optional) Hyperparameter tuning for Logistic Regression\nparam_grid = {\n    'penalty': ['l1', 'l2'],\n    'C': [0.01, 0.1, 1, 10, 100],\n    'solver': ['liblinear']\n}\ngrid_log_reg = GridSearchCV(\n    LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n    param_grid,\n    cv=5,\n    scoring='f1',\n    n_jobs=-1\n)\ngrid_log_reg.fit(X_train_balanced, y_train_balanced)\nprint(\"Best parameters for Logistic Regression:\", grid_log_reg.best_params_)\nprint(\"Best F1 score from GridSearch:\", grid_log_reg.best_score_)\n\nbest_log_reg = grid_log_reg.best_estimator_\ntrain_m, test_m = evaluate_model_full(best_log_reg, \"Logistic Regression (Tuned)\", X_train_scaled, y_train, X_test_scaled, y_test)\nresults_train[\"Logistic Regression (Tuned)\"] = train_m\nresults_test[\"Logistic Regression (Tuned)\"] = test_m\n\n# Adjust decision threshold for tuned logistic model (e.g., threshold = 0.4)\nthreshold = 0.4\nif hasattr(best_log_reg, \"predict_proba\"):\n    y_prob_best_log = best_log_reg.predict_proba(X_test_scaled)[:, 1]\nelse:\n    y_prob_best_log = best_log_reg.predict(X_test_scaled)\ny_pred_thresh = (y_prob_best_log >= threshold).astype(int)\nadj_metrics = {\n    'Accuracy': accuracy_score(y_test, y_pred_thresh),\n    'Precision': precision_score(y_test, y_pred_thresh, zero_division=0),\n    'Recall': recall_score(y_test, y_pred_thresh, zero_division=0),\n    'F1 Score': f1_score(y_test, y_pred_thresh, zero_division=0),\n    'ROC-AUC': roc_auc_score(y_test, y_prob_best_log) if hasattr(best_log_reg, \"predict_proba\") else None\n}\nprint(f\"Logistic Regression (Tuned, Threshold {threshold}) Test Metrics: {adj_metrics}\\n\")\nresults_test[f\"Logistic Regression (Tuned, Threshold {threshold})\"] = adj_metrics\n\n# ------------------------------\n# 4. Graphs for Train and Test Evaluation Metrics\n# ------------------------------\nimport matplotlib.pyplot as plt\n\n# Convert the results dictionaries to DataFrames\ntrain_df = pd.DataFrame(results_train).T\ntest_df = pd.DataFrame(results_test).T\n\n# Define the metrics to plot\nmetrics_list = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\nmodels = train_df.index.tolist()\n\nfor metric in metrics_list:\n    plt.figure(figsize=(10, 6))\n    indices = np.arange(len(models))\n    width = 0.35","metadata":{"_uuid":"a561ca7b-79c0-422d-92d4-aaeed4d5e073","_cell_guid":"e07af0a2-338d-479b-be1e-43174f32911e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-07T06:14:15.820344Z","iopub.execute_input":"2025-04-07T06:14:15.820750Z","iopub.status.idle":"2025-04-07T06:15:56.081050Z","shell.execute_reply.started":"2025-04-07T06:14:15.820712Z","shell.execute_reply":"2025-04-07T06:15:56.079939Z"}},"outputs":[{"name":"stdout","text":"Class distribution before SMOTE: [23721 10166]\nClass distribution after SMOTE: [23721 10166]\nLinear Regression Train Metrics: {'Accuracy': 0.7025112875143861, 'Precision': 0.5457481162540366, 'Recall': 0.049872122762148335, 'F1 Score': 0.09139251915277151, 'ROC-AUC': None}\nLinear Regression Test Metrics: {'Accuracy': 0.7008970727101038, 'Precision': 0.5158730158730159, 'Recall': 0.05114083398898505, 'F1 Score': 0.09305654974946313, 'ROC-AUC': None}\n\nLogistic Regression Train Metrics: {'Accuracy': 0.621772361082421, 'Precision': 0.3900273790757488, 'Recall': 0.4624237654928192, 'F1 Score': 0.42315135694675726, 'ROC-AUC': 0.6033725801540555}\nLogistic Regression Test Metrics: {'Accuracy': 0.6176817752596789, 'Precision': 0.3865190491696516, 'Recall': 0.466955153422502, 'F1 Score': 0.4229467308034919, 'ROC-AUC': 0.5977768763027347}\n\nKNN Train Metrics: {'Accuracy': 0.7518517425561425, 'Precision': 0.651126784792706, 'Recall': 0.37231949636041706, 'F1 Score': 0.4737467926653733, 'ROC-AUC': 0.7786328872340911}\nKNN Test Metrics: {'Accuracy': 0.6453021718602455, 'Precision': 0.3417634996582365, 'Recall': 0.1966955153422502, 'F1 Score': 0.24968789013732834, 'ROC-AUC': 0.5354422763343121}\n\nDecision Tree Train Metrics: {'Accuracy': 0.9982884291911353, 'Precision': 0.9943270735524257, 'Recall': 1.0, 'F1 Score': 0.9971554683668465, 'ROC-AUC': 0.9999949802545482}\nDecision Tree Test Metrics: {'Accuracy': 0.5866383380547686, 'Precision': 0.31859410430839, 'Recall': 0.3316286388670338, 'F1 Score': 0.3249807247494217, 'ROC-AUC': 0.5136157080441499}\n\nRandom Forest Train Metrics: {'Accuracy': 0.9985835276064567, 'Precision': 0.9967596229379418, 'Recall': 0.9985244934094039, 'F1 Score': 0.9976412776412775, 'ROC-AUC': 0.999924197904184}\nRandom Forest Test Metrics: {'Accuracy': 0.6955854579792257, 'Precision': 0.46476190476190476, 'Recall': 0.0959874114870181, 'F1 Score': 0.15911313987610043, 'ROC-AUC': 0.560748232393927}\n\nVoting Classifier Train Metrics: {'Accuracy': 0.9985835276064567, 'Precision': 0.9962723170492447, 'Recall': 0.9990163289396026, 'F1 Score': 0.9976424361493124, 'ROC-AUC': 0.9999881670023573}\nVoting Classifier Test Metrics: {'Accuracy': 0.5872285174693107, 'Precision': 0.3183720045644732, 'Recall': 0.32926829268292684, 'F1 Score': 0.32372848578611485, 'ROC-AUC': 0.55962557532609}\n\nKMeans Clustering Train Metrics: {'Accuracy': 0.7000029509841532, 'Precision': 0.0, 'Recall': 0.0, 'F1 Score': 0.0, 'ROC-AUC': None}\nKMeans Clustering Test Metrics: {'Accuracy': 0.6999527856468366, 'Precision': 0.0, 'Recall': 0.0, 'F1 Score': 0.0, 'ROC-AUC': None} \n\nGaussian Naive Bayes Train Metrics: {'Accuracy': 0.6838610676660666, 'Precision': 0.4399033179520984, 'Recall': 0.1969309462915601, 'F1 Score': 0.27206631786369506, 'ROC-AUC': 0.5882471976115085}\nGaussian Naive Bayes Test Metrics: {'Accuracy': 0.6861425873465533, 'Precision': 0.4492627927146574, 'Recall': 0.2037765538945712, 'F1 Score': 0.2803788903924222, 'ROC-AUC': 0.5779766366858032}\n\nAdaBoost Train Metrics: {'Accuracy': 0.630182665919084, 'Precision': 0.39646420444600033, 'Recall': 0.4456029903600236, 'F1 Score': 0.41959985179696185, 'ROC-AUC': 0.6088302750705226}\nAdaBoost Test Metrics: {'Accuracy': 0.6165014164305949, 'Precision': 0.37889688249400477, 'Recall': 0.43509047993705746, 'F1 Score': 0.40505401941036434, 'ROC-AUC': 0.5851620598564686}\n\nStacking Classifier Train Metrics: {'Accuracy': 0.7968542508926727, 'Precision': 0.620803886925795, 'Recall': 0.82952980523313, 'F1 Score': 0.7101473684210526, 'ROC-AUC': 0.8732136434433793}\nStacking Classifier Test Metrics: {'Accuracy': 0.6109537299339, 'Precision': 0.3792440743113389, 'Recall': 0.46577498033044845, 'F1 Score': 0.4180790960451978, 'ROC-AUC': 0.5960821769317622}\n\nBest parameters for Logistic Regression: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\nBest F1 score from GridSearch: 0.417938043894358\nLogistic Regression (Tuned) Train Metrics: {'Accuracy': 0.6233658925251572, 'Precision': 0.3910562966691837, 'Recall': 0.4584890812512296, 'F1 Score': 0.4220964455512792, 'ROC-AUC': 0.6032142290596145}\nLogistic Regression (Tuned) Test Metrics: {'Accuracy': 0.6199244570349386, 'Precision': 0.3878968253968254, 'Recall': 0.46144767899291894, 'F1 Score': 0.42148760330578516, 'ROC-AUC': 0.5968637182019974}\n\nLogistic Regression (Tuned, Threshold 0.4) Test Metrics: {'Accuracy': 0.3525731822474032, 'Precision': 0.30887128198467334, 'Recall': 0.9354838709677419, 'F1 Score': 0.4644077726784494, 'ROC-AUC': 0.5968637182019974}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 0 Axes>"},"metadata":{}}],"execution_count":15}]}