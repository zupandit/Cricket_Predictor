{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11302346,"sourceType":"datasetVersion","datasetId":7068334},{"sourceId":231971275,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Data Engineering","metadata":{"_uuid":"ecae50c6-67e5-437d-a5fe-34f8cd5f3296","_cell_guid":"fce9a0ea-fe8b-4e18-9084-354b1b4667ae","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install imblearn xgboost torch","metadata":{"_uuid":"7d4883e6-6cb6-40f0-b3b4-586391184076","_cell_guid":"d6123cea-9228-4cd2-916d-efe858b55ed1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, Lasso\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n\n# For oversampling\nfrom imblearn.over_sampling import SMOTE\n\n# For models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\n\n# For neural network (PyTorch)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"_uuid":"0ee6ff48-c867-416d-b6e5-89f4e84f541c","_cell_guid":"616ee0ee-722f-434c-b08c-2d3bfb9d0b5e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize lists and dictionaries for model names and metrics\nmodel_names = [\"Logistic_Regression\", \"Decision_Tree\", \"Random_Forest\", \"XGBoost\", \n               \"SVC\", \"KNN\", \"Naive_Bayes\", \"Neural_Network\", \"Gradient_Boosting\", \"Voting_Classifier\", \"KMeans\"]\n\nmetrics_train = {}\nmetrics_test = {}\nfor model in model_names:\n    for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n        metrics_train[f\"{metric}_{model}\"] = None\n        metrics_test[f\"{metric}_{model}\"] = None\n    for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n        metrics_test[f\"{metric}_kfold_{model}\"] = None\n\nmodels = []      # To store fitted models\nmodels_name = [] # To store their names","metadata":{"_uuid":"0af8f9d7-70d8-40cc-8b4d-f2e07fe2888f","_cell_guid":"673a3ff0-d33e-45b8-8bf8-025cfda941eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset and perform basic cleaning/feature engineering\ndata = pd.read_csv(\"test/over_features.csv\")\ndata = data.iloc[:10000]  # Limit for speed\n\n# Check and drop missing values\nprint(\"Missing values in data:\")\nprint(data.isnull().sum())\ndata.dropna(inplace=True)\n\n# Remove identifier column\ndata = data.drop(columns=[\"match_id\"])\n\n# Feature Engineering: add enhanced features\ndata[\"pressure_index\"] = data[\"dot_ball_pressure\"] * data[\"required_desired_run_rate\"]\ndata[\"wicket_pressure\"] = data[\"number_of_wickets_lost\"] * data[\"required_desired_run_rate\"]\ndata[\"late_over_flag\"] = (data[\"over\"] > 15).astype(int)\ndata[\"bowler_pressure\"] = data[\"current_bowler_economy\"] * (data[\"bowler_wickets_in_match\"] + 1)\ndata[\"aggressiveness_index\"] = data[\"striker_strike_rate\"] * (data[\"striker_boundaries_hit\"] + 1)\n\nprint(\"Number of rows:\", len(data))\nprint(\"Columns:\", data.columns.tolist())\n\n# Encode categorical columns\nteam_mapping = {team: idx for idx, team in enumerate(data[\"team\"].unique())}\ndata[\"team_encoded\"] = data[\"team\"].map(team_mapping)\nphase_mapping = {phase: idx for idx, phase in enumerate(data[\"match_phase\"].unique())}\ndata[\"match_phase_encoded\"] = data[\"match_phase\"].map(phase_mapping)\n\n# Drop original categorical columns and unused features\ndata_encoded = data.drop(columns=[\"team\", \"match_phase\", \"striker_boundaries_hit\", \"total_overs_completed\"])","metadata":{"_uuid":"54164b90-5797-4d0b-99c1-143316cc7ded","_cell_guid":"64d96f81-eddc-4588-9992-e1ea2a3407d5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-test split and feature scaling\ntrain_data, test_data = train_test_split(data_encoded, test_size=0.2, random_state=42)\n\nX_train = train_data.drop(columns=[\"wicket_next_over\"])\ny_train = train_data[\"wicket_next_over\"]\nX_test = test_data.drop(columns=[\"wicket_next_over\"])\ny_test = test_data[\"wicket_next_over\"]\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# -------------------------------\n# Apply SMOTE only on training data\nsmote = SMOTE(random_state=42)\nX_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\nX_train_scaled = X_train_balanced\ny_train = y_train_balanced","metadata":{"_uuid":"445035b3-b7e2-4425-9ea3-8ea03a4c25f3","_cell_guid":"ee29bc5f-d9e9-4b48-ac64-53c570c3ff5c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Selection & Visualization","metadata":{"_uuid":"dc6378f4-90b0-45b5-947c-5299fc5c07a1","_cell_guid":"dd6eb728-66f5-4fd4-89e2-e62930981c6e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Lasso Regression for feature importance\nalphas = np.logspace(-3, 1, 100)\nlasso = Lasso(max_iter=1000)\ngrid = GridSearchCV(lasso, param_grid={\"alpha\": alphas}, cv=5, scoring=\"r2\")\ngrid.fit(X_train_scaled, y_train)\n\nbest_lasso = grid.best_estimator_\nlasso_coef = best_lasso.coef_\n\nplt.figure(figsize=(10, 5))\nplt.bar(X_train.columns, lasso_coef, color='blue')\nplt.axhline(0, color='black', linewidth=0.8, linestyle='--')\nplt.xlabel(\"Features\")\nplt.ylabel(\"Coefficient Value\")\nplt.title(\"Lasso Regression Feature Importance\")\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\nprint(\"Lasso Feature Coefficients:\", dict(zip(X_train.columns, lasso_coef)))","metadata":{"_uuid":"565d9fbf-c62b-4be9-89ea-2f71ed21997c","_cell_guid":"53e7d661-ced8-450b-afb8-dabd954dcfe8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correlation Heatmap\ncorrelation_matrix = X_train.corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Correlation Heatmap')\nplt.show()","metadata":{"_uuid":"6773a66d-cacb-48e7-a6ce-28cab9258fe0","_cell_guid":"5cecf4d2-b60f-4df8-93bc-9dc31d9e77e7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Random Forest Feature Importance\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train_scaled, y_train)\nfeature_importances = rf.feature_importances_\nimportance_df = pd.DataFrame({\n    'Feature': X_train.columns,  \n    'Importance': feature_importances\n}).sort_values(by='Importance', ascending=False)\nprint(\"Random Forest Feature Importances:\")\nprint(importance_df)\n\nplt.figure(figsize=(10,6))\nplt.barh(importance_df['Feature'], importance_df['Importance'])\nplt.xlabel('Feature Importance')\nplt.title('Random Forest Feature Importance')\nplt.show()","metadata":{"_uuid":"fbc4b185-69fc-47f8-a56f-0c8deec7d6ea","_cell_guid":"3a5564d9-e83f-440d-9dac-df982f04e77e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training Functions","metadata":{"_uuid":"53de61c2-d5fe-4207-be10-f67ffc164e5e","_cell_guid":"5665d380-6679-4254-b7ac-dd35495dd1c4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def calculate_metrics(y_true, y_pred):\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n    return accuracy, precision, recall, f1\n\ndef evaluate_model(y_train_true, y_train_pred, y_test_true, y_test_pred, model_name, model):\n    # Store model and name globally\n    models.append(model)\n    models_name.append(model_name)\n    \n    # Calculate metrics and store\n    accuracy_train, precision_train, recall_train, f1_train = calculate_metrics(y_train_true, y_train_pred)\n    accuracy_test, precision_test, recall_test, f1_test = calculate_metrics(y_test_true, y_test_pred)\n    \n    metrics_train[f\"accuracy_{model_name}\"] = accuracy_train\n    metrics_train[f\"precision_{model_name}\"] = precision_train\n    metrics_train[f\"recall_{model_name}\"] = recall_train\n    metrics_train[f\"f1_score_{model_name}\"] = f1_train\n\n    metrics_test[f\"accuracy_{model_name}\"] = accuracy_test\n    metrics_test[f\"precision_{model_name}\"] = precision_test\n    metrics_test[f\"recall_{model_name}\"] = recall_test\n    metrics_test[f\"f1_score_{model_name}\"] = f1_test\n    \n    cross_validate_model(model, model_name, X_train_scaled, y_train)\n\ndef cross_validate_model(model, model_name, X, y, n_splits=5):\n    average_metrics = {\"accuracy\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0}\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    if isinstance(X, np.ndarray):\n        X_df = pd.DataFrame(X, columns=X_train.columns)\n    else:\n        X_df = X.copy()\n    if isinstance(y, np.ndarray):\n        y_series = pd.Series(y)\n    else:\n        y_series = y.copy()\n    \n    k_folds = 0\n    for train_index, test_index in skf.split(X_df, y_series):\n        k_folds += 1\n        X_train_cv = X_df.iloc[train_index]\n        X_test_cv = X_df.iloc[test_index]\n        y_train_cv = y_series.iloc[train_index]\n        y_test_cv = y_series.iloc[test_index]\n        \n        model.fit(X_train_cv, y_train_cv)\n        y_pred_cv = model.predict(X_test_cv)\n        acc, prec, rec, f1 = calculate_metrics(y_test_cv, y_pred_cv)\n        average_metrics[\"accuracy\"] += acc\n        average_metrics[\"precision\"] += prec\n        average_metrics[\"recall\"] += rec\n        average_metrics[\"f1_score\"] += f1\n        \n    metrics_test[f\"accuracy_kfold_{model_name}\"] = average_metrics[\"accuracy\"] / k_folds\n    metrics_test[f\"precision_kfold_{model_name}\"] = average_metrics[\"precision\"] / k_folds\n    metrics_test[f\"recall_kfold_{model_name}\"] = average_metrics[\"recall\"] / k_folds\n    metrics_test[f\"f1_score_kfold_{model_name}\"] = average_metrics[\"f1_score\"] / k_folds\n\ndef train_and_evaluate_model(model, model_name):\n    model.fit(X_train_scaled, y_train)\n    y_train_pred = model.predict(X_train_scaled)\n    y_test_pred = model.predict(X_test_scaled)\n    evaluate_model(y_train, y_train_pred, y_test, y_test_pred, model_name, model)","metadata":{"_uuid":"cab3ed34-8c4d-48da-98c5-45d05819b4a8","_cell_guid":"e87c3934-2d49-4faa-abba-889f6e442c40","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train Various Models","metadata":{"_uuid":"7ccaa8d1-4a67-4d9a-a115-8227f3e9fa53","_cell_guid":"399b9987-e343-4245-8569-f29ba1b4a0dd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Logistic Regression\nmodel = LogisticRegression(multi_class='multinomial', max_iter=1000)\ntrain_and_evaluate_model(model, \"Logistic_Regression\")","metadata":{"_uuid":"e01de10c-3031-4de5-879f-b58403d72163","_cell_guid":"4a73c426-064a-4021-ade5-f315f7b5b8d7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision Tree\nmodel = DecisionTreeClassifier(random_state=42)\ntrain_and_evaluate_model(model, \"Decision_Tree\")","metadata":{"_uuid":"a44e7896-63e7-4d45-9490-70435ccae2d3","_cell_guid":"7a791206-f6d5-46b8-9312-184f3d74057c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Random Forest\nmodel = RandomForestClassifier(random_state=42)\ntrain_and_evaluate_model(model, \"Random_Forest\")","metadata":{"_uuid":"13bf2f2d-52c9-4cd6-883e-f1ff2f982600","_cell_guid":"c3d437ea-381b-4b95-938b-6d36a86539a2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gradient Boosting\nmodel = GradientBoostingClassifier(random_state=42)\ntrain_and_evaluate_model(model, \"Gradient_Boosting\")","metadata":{"_uuid":"758dfcd2-c92c-4055-a901-46dcd83465f3","_cell_guid":"c4ef4939-9f1e-4c13-bd3f-e17b6b6a7c4c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SVC\nmodel = SVC(random_state=42)\ntrain_and_evaluate_model(model, \"SVC\")","metadata":{"_uuid":"5bf3ea1f-77e0-4ae2-9f36-5ba10bba41bc","_cell_guid":"1f21ba11-7457-4327-8539-f2d484d5cd9c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# KMeans\nmodel = KMeans(n_clusters=2, random_state=42)\ntrain_and_evaluate_model(model, \"KMeans\")","metadata":{"_uuid":"b2ed14dc-a839-419d-b22b-45f5ad4d1021","_cell_guid":"d830c537-86d4-4c9e-b453-30f9bb1cd801","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# KNN\nmodel = KNeighborsClassifier(n_neighbors=5)\ntrain_and_evaluate_model(model, \"KNN\")","metadata":{"_uuid":"f0d321c6-b46f-4451-9462-4da016caab3b","_cell_guid":"809c2fc5-f393-44e1-89c8-070f52cddc6b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Naive Bayes\nmodel = GaussianNB()\ntrain_and_evaluate_model(model, \"Naive_Bayes\")","metadata":{"_uuid":"134a6c46-1334-425d-b9b4-9aff7394dc4e","_cell_guid":"fc78284a-3f81-4322-ab09-de2a9141b7f3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# XGBoost\nmodel = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\ntrain_and_evaluate_model(model, \"XGBoost\")","metadata":{"_uuid":"13202a44-9a46-4a7c-92ea-6bc948ccc0bf","_cell_guid":"5048cd1f-024c-4255-81e7-1fde5d1b3ec0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Voting Classifier\nmodel = VotingClassifier(estimators=[\n    ('logistic', LogisticRegression(multi_class='multinomial', max_iter=1000)),\n    ('tree', DecisionTreeClassifier(random_state=42)),\n    ('forest', RandomForestClassifier(random_state=42)),\n    ('boosting', GradientBoostingClassifier(random_state=42)),\n    ('xgboost', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))\n], voting='soft')\ntrain_and_evaluate_model(model, \"Voting_Classifier\")","metadata":{"_uuid":"71100862-db78-4c72-acaf-538ba0651579","_cell_guid":"dd5c78dc-b705-4aa7-a6ed-7c0d6e15de56","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Neural Network Model (PyTorch)","metadata":{"_uuid":"83d1b7c3-1b69-4c3b-ab97-59d4b810ebfc","_cell_guid":"e87649be-66b5-4195-8cc7-a4e3864ef699","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Define the neural network model\nnum_features = X_train.shape[1]\nclass ImprovedWicketPredictor(nn.Module):\n    def __init__(self, input_dim):\n        super(ImprovedWicketPredictor, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 16)\n        self.fc4 = nn.Linear(16, 8)\n        self.fc5 = nn.Linear(8, 1)\n\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.dropout = nn.Dropout(p=0.5)\n\n        self.batch_norm1 = nn.BatchNorm1d(64)\n        self.batch_norm2 = nn.BatchNorm1d(32)\n        self.batch_norm3 = nn.BatchNorm1d(16)\n        self.batch_norm4 = nn.BatchNorm1d(8)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.batch_norm1(x)\n        x = self.dropout(x)\n\n        x = self.relu(self.fc2(x))\n        x = self.batch_norm2(x)\n        x = self.dropout(x)\n\n        x = self.relu(self.fc3(x))\n        x = self.batch_norm3(x)\n        x = self.dropout(x)\n\n        x = self.relu(self.fc4(x))\n        x = self.batch_norm4(x)\n        x = self.fc5(x)\n        x = self.sigmoid(x)\n        return x\n\ndef NN_predict(X_train_df, X_test_df, Y_train_df, Y_test_df):\n    X_train_tensor = torch.tensor(X_train_df.to_numpy(), dtype=torch.float32)\n    X_test_tensor = torch.tensor(X_test_df.to_numpy(), dtype=torch.float32)\n    y_train_tensor = torch.tensor(Y_train_df.to_numpy(), dtype=torch.float32).view(-1, 1)\n    y_test_tensor = torch.tensor(Y_test_df.to_numpy(), dtype=torch.float32).view(-1, 1)\n\n    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n\n    input_dim = X_train_df.shape[1]\n    model_nn = ImprovedWicketPredictor(input_dim)\n\n    criterion = nn.BCELoss()\n    optimizer = optim.AdamW(model_nn.parameters(), lr=0.001)\n\n    epochs = 100\n    best_accuracy = 0\n    patience = 10\n    no_improvement = 0\n\n    for epoch in range(epochs):\n        model_nn.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model_nn(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        with torch.no_grad():\n            model_nn.eval()\n            y_train_pred = model_nn(X_train_tensor).round()\n            accuracy_val = accuracy_score(y_train_tensor.numpy(), y_train_pred.numpy())\n            if (epoch + 1) % 10 == 0:\n                print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Training Accuracy: {accuracy_val:.4f}')\n        if accuracy_val > best_accuracy:\n            best_accuracy = accuracy_val\n            no_improvement = 0\n        else:\n            no_improvement += 1\n        if no_improvement >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n    model_nn.eval()\n    with torch.no_grad():\n        y_test_pred = model_nn(X_test_tensor).round()\n    return y_test_pred\n\ndef cross_validate_NN(model_name, X, y, n_splits=5):\n    average_metrics = {\"accuracy\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0}\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    if not isinstance(X, pd.DataFrame):\n        X = pd.DataFrame(X, columns=X_train.columns)\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    \n    k_folds = 0\n    for train_index, test_index in skf.split(X, y):\n        k_folds += 1\n        X_train_cv = X.iloc[train_index]\n        X_test_cv = X.iloc[test_index]\n        Y_train_cv = y.iloc[train_index]\n        Y_test_cv = y.iloc[test_index]\n        \n        y_pred_cv = NN_predict(X_train_cv, X_test_cv, Y_train_cv, Y_test_cv)\n        y_pred_binary = (y_pred_cv > 0.5).astype(int)\n        acc, prec, rec, f1 = calculate_metrics(Y_test_cv, y_pred_binary)\n        average_metrics[\"accuracy\"] += acc\n        average_metrics[\"precision\"] += prec\n        average_metrics[\"recall\"] += rec\n        average_metrics[\"f1_score\"] += f1\n    \n    metrics_test[f\"accuracy_kfold_{model_name}\"] = average_metrics[\"accuracy\"] / k_folds\n    metrics_test[f\"precision_kfold_{model_name}\"] = average_metrics[\"precision\"] / k_folds\n    metrics_test[f\"recall_kfold_{model_name}\"] = average_metrics[\"recall\"] / k_folds\n    metrics_test[f\"f1_score_kfold_{model_name}\"] = average_metrics[\"f1_score\"] / k_folds\n\n# Evaluate Neural Network on full data (for demonstration)\nX_all_data = data_encoded.drop(columns=[\"wicket_next_over\"])\nY_all_data = data_encoded[\"wicket_next_over\"]\ny_pred_nn = NN_predict(X_all_data, X_all_data, Y_all_data, Y_all_data)\nacc_nn, prec_nn, rec_nn, f1_nn = calculate_metrics(Y_all_data, y_pred_nn)\nprint(f'Neural Network on All Data: Accuracy: {acc_nn*100:.2f}%, Precision: {prec_nn*100:.2f}%, Recall: {rec_nn*100:.2f}%, F1 Score: {f1_nn*100:.2f}%')\n\ncross_validate_NN(\"Neural_Network\", X_all_data, Y_all_data, n_splits=5)\nprint(\"Neural Network Cross-validation metrics:\")\nfor metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n    print(f\"{metric}_Neural_Network: {metrics_test[f'{metric}_kfold_Neural_Network']:.4f}\")\n\n# Evaluate NN on training set\nX_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\ny_pred_train_nn = NN_predict(X_train_df, X_train_df, y_train, y_train)\nacc_train_nn, prec_train_nn, rec_train_nn, f1_train_nn = calculate_metrics(y_train, y_pred_train_nn)\nprint(f'Neural Network on Train Data: Accuracy: {acc_train_nn*100:.2f}%, Precision: {prec_train_nn*100:.2f}%, Recall: {rec_train_nn*100:.2f}%, F1 Score: {f1_train_nn*100:.2f}%')\nmetrics_train[\"accuracy_Neural_Network\"] = acc_train_nn\nmetrics_train[\"precision_Neural_Network\"] = prec_train_nn\nmetrics_train[\"recall_Neural_Network\"] = rec_train_nn\nmetrics_train[\"f1_score_Neural_Network\"] = f1_train_nn\n\n# Evaluate NN on test set\nX_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\ny_pred_test_nn = NN_predict(X_test_df, X_test_df, y_test, y_test)\nacc_test_nn, prec_test_nn, rec_test_nn, f1_test_nn = calculate_metrics(y_test, y_pred_test_nn)\nmetrics_test[\"accuracy_Neural_Network\"] = acc_test_nn\nmetrics_test[\"precision_Neural_Network\"] = prec_test_nn\nmetrics_test[\"recall_Neural_Network\"] = rec_test_nn\nmetrics_test[\"f1_score_Neural_Network\"] = f1_test_nn\nprint(f'Neural Network on Test Data: Accuracy: {acc_test_nn*100:.2f}%, Precision: {prec_test_nn*100:.2f}%, Recall: {rec_test_nn*100:.2f}%, F1 Score: {f1_test_nn*100:.2f}%')","metadata":{"_uuid":"50134529-02c4-467c-ab25-9599c3a7cdc9","_cell_guid":"b29bbf2c-84b9-43e6-96cf-1205777cf2b7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Updated Hyperparameter Tuning to Fix Overfitting","metadata":{"_uuid":"b4dad528-89f8-4fe1-a541-b19ed7897608","_cell_guid":"200c2bd4-c023-4c23-947c-e3c8f656e416","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def under_or_overfitting(model, adjustment_type, model_name):\n    \"\"\"\n    Adjust model hyperparameters to address overfitting or underfitting.\n    For most models, overfitting => reduce complexity (sign = -1)\n    and underfitting => increase complexity (sign = +1).\n    For KNN, the adjustment is reversed.\n    \"\"\"\n    # For most models:\n    sign = -1 if adjustment_type == \"overfitting\" else 1\n\n    try:\n        params = model.get_params()\n    except:\n        return model\n\n    # GENERAL ADJUSTMENTS\n    if 'max_iter' in params and (params['max_iter'] + (100 * sign) > 0):\n        model.set_params(max_iter=params['max_iter'] + (100 * sign))\n        \n    if 'penalty' in params:\n        if adjustment_type == \"overfitting\":\n            model.set_params(penalty='l1' if params['penalty'] == 'l2' else 'l2')\n\n    # LOGISTIC REGRESSION: Adjust C\n    if model_name == \"Logistic_Regression\" and 'C' in params:\n        new_C = params['C'] + (0.1 * sign)\n        if new_C > 0:\n            model.set_params(C=new_C)\n\n    # SVC adjustments\n    if model_name == \"SVC\":\n        if 'kernel' in params:\n            kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n            current_kernel = params['kernel']\n            next_index = (kernels.index(current_kernel) + sign) % len(kernels)\n            model.set_params(kernel=kernels[next_index])\n        if 'gamma' in params and isinstance(params['gamma'], (int, float)):\n            new_gamma = params['gamma'] + (0.01 * sign)\n            if new_gamma > 0:\n                model.set_params(gamma=new_gamma)\n\n    # TREE-BASED MODELS: Decision Tree, Random Forest, Gradient Boosting\n    if model_name in [\"Decision_Tree\", \"Random_Forest\", \"Gradient_Boosting\"]:\n        if 'max_depth' in params:\n            depth = params['max_depth'] if params['max_depth'] is not None else 5\n            new_depth = depth + (3 * sign)\n            if new_depth > 0:\n                model.set_params(max_depth=new_depth)\n        if 'min_samples_split' in params:\n            val = params['min_samples_split'] if params['min_samples_split'] is not None else 2\n            new_val = val + (2 * sign)\n            if new_val > 0:\n                model.set_params(min_samples_split=new_val)\n        if 'min_samples_leaf' in params:\n            val = params['min_samples_leaf'] if params['min_samples_leaf'] is not None else 1\n            # For overfitting, increase min_samples_leaf\n            new_val = val + (2 if adjustment_type == \"overfitting\" else -2)\n            if new_val < 1:\n                new_val = 1\n            model.set_params(min_samples_leaf=new_val)\n        if 'max_features' in params:\n            val = params['max_features']\n            if isinstance(val, (int, float)):\n                model.set_params(max_features=val + (1 * sign))\n        if 'max_leaf_nodes' in params:\n            val = params['max_leaf_nodes'] if params['max_leaf_nodes'] is not None else 10\n            new_val = val + (5 * sign)\n            if new_val > 0:\n                model.set_params(max_leaf_nodes=new_val)\n        if 'criterion' in params:\n            options = ['gini', 'entropy', 'log_loss']\n            if params['criterion'] in options:\n                next_index = (options.index(params['criterion']) + sign) % len(options)\n                model.set_params(criterion=options[next_index])\n        if 'splitter' in params:\n            splitters = ['best', 'random']\n            current = params['splitter']\n            model.set_params(splitter=splitters[(splitters.index(current) + sign) % len(splitters)])\n        if 'bootstrap' in params:\n            model.set_params(bootstrap=not params['bootstrap'])\n        if 'n_estimators' in params:\n            val = params['n_estimators'] if params['n_estimators'] is not None else 100\n            new_val = val + (10 * sign)\n            if new_val > 0:\n                model.set_params(n_estimators=new_val)\n\n    # KNN adjustments (reverse the sign for overfitting: increase neighbors)\n    if model_name == \"KNN\":\n        knn_sign = 1 if adjustment_type == \"overfitting\" else -1\n        if 'n_neighbors' in params:\n            new_neighbors = params['n_neighbors'] + (1 * knn_sign)\n            if new_neighbors > 0:\n                model.set_params(n_neighbors=new_neighbors)\n        if 'weights' in params:\n            options = ['uniform', 'distance']\n            model.set_params(weights=options[(options.index(params['weights']) + knn_sign) % len(options)])\n        if 'algorithm' in params:\n            algorithms = ['auto', 'ball_tree', 'kd_tree', 'brute']\n            model.set_params(algorithm=algorithms[(algorithms.index(params['algorithm']) + knn_sign) % len(algorithms)])\n\n    # KMeans adjustments\n    if model_name == \"KMeans\":\n        if 'n_clusters' in params:\n            new_clusters = params['n_clusters'] + (1 * sign)\n            if new_clusters > 1:\n                model.set_params(n_clusters=new_clusters)\n        if 'init' in params:\n            inits = ['k-means++', 'random']\n            model.set_params(init=inits[(inits.index(params['init']) + sign) % len(inits)])\n\n    # XGBoost adjustments\n    if model_name == \"XGBoost\":\n        if 'subsample' in params:\n            val = params['subsample'] if params['subsample'] is not None else 1.0\n            new_subsample = val + (0.05 * sign)\n            if 0 < new_subsample <= 1:\n                model.set_params(subsample=new_subsample)\n        if 'colsample_bytree' in params:\n            val = params['colsample_bytree'] if params['colsample_bytree'] is not None else 1.0\n            new_val = val + (0.05 * sign)\n            if 0 < new_val <= 1:\n                model.set_params(colsample_bytree=new_val)\n        if 'gamma' in params:\n            val = params['gamma'] if params['gamma'] is not None else 0.0\n            new_gamma = val + (0.1 * sign)\n            if new_gamma >= 0:\n                model.set_params(gamma=new_gamma)\n                \n    return model\n\ndef hyperparameter_tuning(metrics_train, metrics_test, models, models_name, max_tuning_iterations=10):\n    \"\"\"\n    For each model, if test accuracy is low (< 0.8) or if training is much higher than test,\n    adjust hyperparameters to reduce overfitting or increase model complexity.\n    \"\"\"\n    for i in range(len(models)):\n        model = models[i]\n        model_name = models_name[i]\n        \n        # Initial evaluation\n        model.fit(X_train_scaled, y_train)\n        y_train_pred = model.predict(X_train_scaled)\n        y_test_pred = model.predict(X_test_scaled)\n        current_train_acc = accuracy_score(y_train, y_train_pred)\n        current_test_acc = accuracy_score(y_test, y_test_pred)\n        print(f\"Initial {model_name}: Train Accuracy={current_train_acc:.4f}, Test Accuracy={current_test_acc:.4f}\")\n        \n        iteration = 0\n        improved = True\n        while iteration < max_tuning_iterations and improved:\n            old_train_acc = current_train_acc\n            old_test_acc = current_test_acc\n            \n            # Overfitting: high train acc but low test acc\n            if current_train_acc > 0.8 and current_test_acc < 0.8:\n                model = under_or_overfitting(model, \"overfitting\", model_name)\n            # Underfitting: test accuracy low\n            elif current_test_acc < 0.8:\n                model = under_or_overfitting(model, \"underfitting\", model_name)\n            else:\n                break\n            \n            model.fit(X_train_scaled, y_train)\n            y_train_pred = model.predict(X_train_scaled)\n            y_test_pred = model.predict(X_test_scaled)\n            current_train_acc = accuracy_score(y_train, y_train_pred)\n            current_test_acc = accuracy_score(y_test, y_test_pred)\n            print(f\"Iteration {iteration+1} for {model_name}: Train Accuracy={current_train_acc:.4f}, Test Accuracy={current_test_acc:.4f}\")\n            \n            if current_train_acc <= old_train_acc and current_test_acc <= old_test_acc:\n                improved = False\n            iteration += 1\n        \n        # Save updated model and re-evaluate\n        evaluate_model(y_train, y_train_pred, y_test, y_test_pred, model_name, model)\n\n# Run hyperparameter tuning for all models (except the NN which uses a separate process)\nhyperparameter_tuning(metrics_train, metrics_test, models, models_name)","metadata":{"_uuid":"5c192b0d-8881-4c73-91af-ab008f897929","_cell_guid":"d9efd62d-04e8-45fa-b1b7-cd20284f8475","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Graphing the Metrics","metadata":{"_uuid":"1d337c38-8df9-445f-8e74-5db806bfb2de","_cell_guid":"86f79d4a-0e55-4737-929d-1d3ef5bce8ca","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def graph_models(metrics_train, metrics_test, models_name):\n    models_to_graph = models_name.copy()\n    metrics_list = [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n    colors = [\"blue\", \"green\", \"orange\", \"red\", \"purple\", \"brown\", \"pink\", \"gray\", \"cyan\", \"magenta\", \"yellow\"]\n    \n    num_metrics = len(metrics_list)\n    num_models = len(models_to_graph)\n    width = 0.15  \n    group_spacing = 2.9\n    x = np.arange(num_metrics) * group_spacing\n\n    # Training Metrics\n    print(\"Training Metrics:\", metrics_train)\n    plt.figure(figsize=(12, 6))\n    for i, model in enumerate(models_to_graph):\n        values = [metrics_train[f\"{metric}_{model}\"] for metric in metrics_list]\n        bar_positions = x + i * width\n        plt.bar(bar_positions, values, width, label=model.replace(\"_\", \" \").title(), color=colors[i % len(colors)])\n    plt.xlabel('Metrics')\n    plt.ylabel('Scores')\n    plt.title('Training Metrics Comparison')\n    plt.xticks(x + (width * (num_models - 1)) / 2, [m.title() for m in metrics_list])\n    plt.ylim(0, 1)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n    # Testing Metrics\n    print(\"Testing Metrics:\", metrics_test)\n    plt.figure(figsize=(12, 6))\n    for i, model in enumerate(models_to_graph):\n        values = [metrics_test[f\"{metric}_{model}\"] for metric in metrics_list]\n        bar_positions = x + i * width\n        plt.bar(bar_positions, values, width, label=model.replace(\"_\", \" \").title(), color=colors[i % len(colors)])\n    plt.xlabel('Metrics')\n    plt.ylabel('Scores')\n    plt.title('Testing Metrics Comparison')\n    plt.xticks(x + (width * (num_models - 1)) / 2, [m.title() for m in metrics_list])\n    plt.ylim(0, 1)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n    \n    # KFold Metrics\n    plt.figure(figsize=(12, 6))\n    for i, model in enumerate(models_to_graph):\n        values = [metrics_test[f\"{metric}_kfold_{model}\"] for metric in metrics_list]\n        bar_positions = x + i * width\n        plt.bar(bar_positions, values, width, label=model.replace(\"_\", \" \").title(), color=colors[i % len(colors)])\n    plt.xlabel('Metrics')\n    plt.ylabel('Scores')\n    plt.title('KFold Metrics Comparison')\n    plt.xticks(x + (width * (num_models - 1)) / 2, [m.title() for m in metrics_list])\n    plt.ylim(0, 1)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\ngraph_models(metrics_train, metrics_test, models_name)","metadata":{"_uuid":"e7d06daa-5836-42fe-97cd-4a630b60da3b","_cell_guid":"8b49ef87-ee9b-4eb2-b4fa-71b17fcd17ef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}